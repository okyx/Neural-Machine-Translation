# -*- coding: utf-8 -*-
"""NMT INDONESIA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DpMpQb9qX-vXvpbeSZeYc9l9Hzd4yZmE
"""

import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import Dense, GRU
import pandas as pd
import string
import re
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split

from tensorflow.keras.preprocessing.sequence import pad_sequences

data = pd.read_table('/content/sample_data/ind.txt', names=['eng', 'ind'],index_col=False)

data

"""cleaning"""

data.eng = data.eng.apply(lambda x : x.lower())
data.ind = data.ind.apply(lambda x : x.lower())

data.head()

string.punctuation

data.eng = data.eng.apply(lambda x : x.translate(str.maketrans('', '', string.punctuation)))
data.ind = data.ind.apply(lambda x : x.translate(str.maketrans('', '', string.punctuation)))

data.eng = data.eng.apply(lambda x : re.sub("'","",x))
data.ind = data.ind.apply(lambda x : re.sub("'","",x))

data.eng = data.eng.apply(lambda x : re.sub(r'[0-9]+','',x))
data.ind = data.ind.apply(lambda x : re.sub(r'[0-9]+','',x))

data

data.eng = data.eng.apply(lambda x : 'START '+ x + ' END')
data.ind = data.ind.apply(lambda x : 'START '+ x + ' END')

maxenLan = max(data.eng.apply(lambda x : len(x.split())).tolist())
maxinLan = max(data.ind.apply(lambda x : len(x.split())).tolist())

print(maxenLan,maxinLan)

maxfiturEn = set()
maxfiturIn = set()

maxfiturEn

data.eng.str.split().apply(maxfiturEn.update)

data.ind.str.split().apply(maxfiturIn.update)

lenVocabEn = len(maxfiturEn)
lenVocabIn = len(maxfiturIn)

print(lenVocabEn,lenVocabIn)

tokenizerIn = Tokenizer(num_words= lenVocabIn, oov_token='-')
tokenizerEn = Tokenizer(num_words= lenVocabEn, oov_token='-')

tokenizerIn.fit_on_texts(data.ind.values)
tokenizerEn.fit_on_texts(data.eng.values)

datain = tokenizerIn.texts_to_sequences(data.ind.values)
dataeng = tokenizerEn.texts_to_sequences(data.eng.values)

XX = pad_sequences(dataeng,padding='post')
YY = pad_sequences(datain,padding='post')

tes = tokenizerEn.texts_to_sequences(np.array(['START you are my cup of tea END']))

tes = pad_sequences(tes,padding='post',maxlen=maxenLan)

tes.shape

tokenizerEn.word_index.items()

X_train,X_test,Y_train,Y_test = train_test_split(XX,YY,test_size=0.2,random_state=2)

def generate_data(X=X_train,Y=Y_train,batch=128):
  while 1:
    for j in range(0,len(X),batch):
      inputenc = X[j:j+batch,:]
      inputdec    = Y[j:j+batch,:-1]
      outputdec = Y[j:j+batch,1:]
      output = np.zeros((outputdec.shape[0],maxinLan-1,lenVocabIn))
      for a in range(outputdec.shape[0]):
        for index,b in enumerate(outputdec[a]):
          output[a][index][b] = 1
      yield ([np.array(inputenc),np.array(inputdec)],output)

"""model

la
"""

laten = 50

"""model encoder"""

InputEnc = tf.keras.layers.Input(shape=(None,),name='inputenc')
embed = tf.keras.layers.Embedding(lenVocabEn,laten,mask_zero = True,name='embedenc')(InputEnc)
gru = tf.keras.layers.GRU(laten,return_state=True,name='gruenc')
out,hid = gru(embed)
states = [hid]

InputDec = tf.keras.layers.Input(shape=(None,),name='inputdec')
embed1 = tf.keras.layers.Embedding(lenVocabIn,laten,mask_zero = True,name='embeddec')
decembed = embed1(InputDec)

gru1= tf.keras.layers.GRU(laten,return_sequences=True,return_state=True,name='grudec')
o,s = gru1(decembed,initial_state = states)
denseLayer = tf.keras.layers.Dense(lenVocabIn,activation='softmax')
dense = denseLayer(o)

model = tf.keras.models.Model([InputEnc,InputDec],dense)

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])

model.summary()

train_samples = len(X_train)
val_samples = len(X_test)
batch_size = 128
epochs = 200

model.fit_generator(generator = generate_data(X_train, Y_train, batch = batch_size),
                    steps_per_epoch = train_samples//batch_size,
                    epochs=epochs,
                    validation_data = generate_data(X_test, Y_test, batch= batch_size),
                    validation_steps = val_samples//batch_size)

"""model decoder"""

model.save_weights('nmt_weights.h5')

encModel = tf.keras.models.Model(InputEnc,states)

o,s= gru1(decembed,initial_state = states)
dense = denseLayer(o)
decoder_model = tf.keras.models.Model(
    [InputDec,states],
    [dense,s]
)

latent_dim=50

decoder_model.summary()

worden =  tokenizerEn.word_index
wordin = tokenizerIn.word_index

wordens = {v: k for k, v in worden.items()}
wordins = {v: k for k, v in wordin.items()}

def translate(Xin):
  for i in Xin[0]:
    if i == 0 :
      break
    if not (wordens[i] == 'start' or wordens[i] == 'end'):
      print(wordens[i],end=" ")
  print()
  print('terjemahan')
  state = encModel.predict(Xin)
  indexof = np.array(tokenizerIn.texts_to_sequences(np.array(['start'])))
  stop_condition = False
  decoded_sentence = ''
  state = state
  while not stop_condition:
      o, h = decoder_model.predict([indexof,state])
      # Sample a token
      sampled_token_index = np.argmax(o[0][0])
      sampled_char = wordins[sampled_token_index]
      decoded_sentence += ' '+sampled_char

      # Exit condition: either hit max length
      # or find stop character.
      if (sampled_char == 'end' or
          len(decoded_sentence) >50):
          stop_condition = True
      # Update the target sequence (of length 1).
      # target_seq = np.zeros((1,1))
      # target_seq[0, 0] = sampled_token_index
      indexof = np.zeros((1,1))
      indexof[0, 0] = sampled_token_index

      # Update states
      state= h

  return decoded_sentence

a = generate_data(batch=100)

b = next(a)

Xin = np.expand_dims(b[0][0][10],axis=0)

Xin.shape

translate(Xin)

translate(tes)